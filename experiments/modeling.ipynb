{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8442e51f-64ff-46f9-adc9-646ef68b6bfe",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387357f4-d27f-4b10-9b2e-6fc8710e1f4a",
   "metadata": {},
   "source": [
    "## Model Input / Output structure\n",
    "\n",
    "### Time Dimension\n",
    "\n",
    "Do I have to use time from `datetime` or `local_solar_time` as time?\n",
    "\n",
    "- `local_solar_time` intervals are 10 minutes\n",
    "- `datetime` intervals are 30 minutes\n",
    "- real time dimension is `datetime`\n",
    "\n",
    "The `local_solar_time` works as one more feature. It is like one more coordinate for the density measurement.\n",
    "\n",
    "I assume the `density` measurements are done on `datetime` time scale.\n",
    "So, I'll use `datetime` for sampling.\n",
    "\n",
    "\n",
    "### Input\n",
    "\n",
    "It is a time series.\n",
    "\n",
    "- Each element of this time series is a matrix on the time dimension, i.e.:\n",
    "  data_set: Sample(t_0), Sample(t_1),..., Sample(t_n),..., Sample(t_max)\n",
    "  where t_x is the `datetime` value (or just an index. See below.).\n",
    "- Since all time intervals (t_delta) between samples are the same (30 min) and no samples are missing, I can use a Sequence with int indexing to access the individual samples. No need to use the `datetime` as index.\n",
    "- Because time is an important feature for density, I could use a year, a numerical day [0, 364/365], and an hour [0, 23] as additional features. TODO Evaluate this hypothesis.\n",
    "- I have 48 samples per file. \n",
    "- A sample presented by (19 * 73 * 137)=190019 rows, by the `density` value.\n",
    "- Files with len != 190019*48 rows are removed from training and evaluation.\n",
    "\n",
    "#### Input reduction\n",
    "\n",
    "I have a \"stable\" file structure. That means:\n",
    "- all files with a stable number of rows (9120912) have a stable structure.\n",
    "- all elements in these fields, ['datetime' (time part of it), 'latitude',\t'longitude', 'altitude', 'local_solar_time'] are equal (in values and positions) in all \"stable\" files.\n",
    "\n",
    "That means I could use rows and fields of the existing \"stable\" files without sorting.\n",
    "I can use only `density` filed in the Input/Output Samples for training and inference. Restoring other fields of the Samples can be simply done.\n",
    "Since removing other fields from file does not reduce the file size (because of the `parquet` effective compression), I do not remove other files from files.\n",
    "\n",
    "- I reduce the input size and the snapshot dimension for faster training of the baseline model to the 'snapshot_size' value.\n",
    "\n",
    "### Output\n",
    "\n",
    "- Output is a vector of 'snapshot_size' predicted density values, the `density_predicted`.\n",
    "\n",
    "### Autoregressive cycle\n",
    "\n",
    "It is executed after model inferences/predicts the Output.\n",
    "\n",
    "- Sample(t_max+1):\n",
    "  - Sample(t_max+1) = Sample(t_max)\n",
    "  - for row in sample_rows [Optional. It is needed only to present the predicted data.]:\n",
    "      - `hour` is incremented by t_delta\n",
    "      - if `hour` == `hours_in_day`: # day increase\n",
    "        - `day` += 1\n",
    "        - `hour` = 0\n",
    "      - latitude, longitude, altitude, local_solar_time copied from Sample(t_max)\n",
    "      - `density` = `density_predicted`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173e2f10-fb4c-4d04-a2cd-36ec8c3b9ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T20:16:44.207655Z",
     "iopub.status.busy": "2024-10-20T20:16:44.205648Z",
     "iopub.status.idle": "2024-10-20T20:16:44.219011Z",
     "shell.execute_reply": "2024-10-20T20:16:44.217047Z",
     "shell.execute_reply.started": "2024-10-20T20:16:44.207589Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b32e234-eff1-4c79-83cb-2a60543b9953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T20:21:48.317887Z",
     "iopub.status.busy": "2024-10-20T20:21:48.317337Z",
     "iopub.status.idle": "2024-10-20T20:21:48.322297Z",
     "shell.execute_reply": "2024-10-20T20:21:48.321456Z",
     "shell.execute_reply.started": "2024-10-20T20:21:48.317840Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "SAMPLES_IN_FILE = 48\n",
    "ROWS_IN_FILE = SAMPLE_SIZE * SAMPLES_IN_FILE\n",
    "\n",
    "INPUT_DIR = '../data/reduced/2000'\n",
    "INPUT_FILE_PATTERN = '2000-*.parquet'\n",
    "DATA_FIELDS = ['datetime', 'density']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294de83-8e2b-45fd-a466-c40331d6f14c",
   "metadata": {},
   "source": [
    "## Data preprocessing v2. TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12b5ebca-d270-4a36-9d54-5259e0090c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:07:42.023595Z",
     "iopub.status.busy": "2024-10-20T22:07:42.022003Z",
     "iopub.status.idle": "2024-10-20T22:07:42.038151Z",
     "shell.execute_reply": "2024-10-20T22:07:42.037687Z",
     "shell.execute_reply.started": "2024-10-20T22:07:42.023513Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_parquet(parquet_files, verbose=False):\n",
    "    \"\"\"load and preprocess data from Parquet files.\"\"\"\n",
    "    data_frames = []\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file, columns=DATA_FIELDS)\n",
    "        if verbose:\n",
    "            print(f\"  Loaded {file} {df.shape}\")\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        data_frames.append(df)\n",
    "    combined_df = pd.concat(data_frames).reset_index(drop=True)\n",
    "    return combined_df\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"Create input sequences and their respective targets (shifted by one step).\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequence = data[i:i + seq_length]\n",
    "        target = data[i + seq_length]  # The next time step\n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "def get_dataloader(sequence_length=SAMPLES_IN_FILE, batch_size=32, tensor_dtype=torch.float32, verbose=False, files=None):\n",
    "    # Load the data from files\n",
    "    parquet_files = glob.glob(f\"{INPUT_DIR}/{INPUT_FILE_PATTERN}\")[:files]\n",
    "    data = load_data_from_parquet(parquet_files)\n",
    "    print(f\"Loaded {len(parquet_files)} files, {data.shape}\")\n",
    "    \n",
    "    # Extracting 'density' values as input samples, and creating shifted targets\n",
    "    density_values = data['density'].values.reshape(-1, SAMPLE_SIZE)  # Reshape into (n_samples, SAMPLE_SIZE)\n",
    "    if verbose:\n",
    "        print(f\"  {density_values.shape=}\")\n",
    "    \n",
    "    # Normalize the density values\n",
    "    scaler = MinMaxScaler()\n",
    "    density_normalized = scaler.fit_transform(density_values)\n",
    "    print(f\"Normalized {len(density_normalized) = }, {density_normalized[0] = }\")\n",
    "    \n",
    "    sequences, targets = create_sequences(density_normalized, sequence_length)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    sequences = torch.tensor(sequences, dtype=tensor_dtype)\n",
    "    targets = torch.tensor(targets, dtype=tensor_dtype)\n",
    "    if verbose:\n",
    "        print(f\"  {sequences.shape=}\")\n",
    "        print(f\"  {targets.shape=}\")\n",
    "    \n",
    "    # Create DataLoader without shuffling\n",
    "    dataset = TensorDataset(sequences, targets)\n",
    "    print(f\"Created TensorDataset: {dataset}\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    print(f\"Created DataLoader: {dataloader}, {batch_size=}\")\n",
    "    return dataloader, density_normalized, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbfb0597-3b39-454a-9f33-80d48d6eb186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:08:16.053144Z",
     "iopub.status.busy": "2024-10-20T22:08:16.052156Z",
     "iopub.status.idle": "2024-10-20T22:08:16.585004Z",
     "shell.execute_reply": "2024-10-20T22:08:16.584579Z",
     "shell.execute_reply.started": "2024-10-20T22:08:16.053093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................Loaded 50 files, (12000000, 2)\n",
      "Normalized len(density_normalized) = 1200, density_normalized[0] = array([1.0058001e-08, 5.5147993e-09, 3.2282998e-09, ..., 2.8465999e-12,\n",
      "       2.6720000e-12, 2.5087001e-12], dtype=float32)\n",
      "Created TensorDataset: <torch.utils.data.dataset.TensorDataset object at 0x7589bd70bee0>\n",
      "Created DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x7589bd708070>, batch_size=4\n",
      "len(dataloader) = 300\n",
      "0\n",
      " sequences_batch.shape =torch.Size([4, 3, 10000])\n",
      " targets_batch.shape =torch.Size([4, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check:\n",
    "\n",
    "dataloader, density_normalized, scaler = get_dataloader(sequence_length=3, batch_size=4, tensor_dtype=torch.float32, verbose=False, files=50)\n",
    "print(f\"{len(dataloader) = }\")\n",
    "\n",
    "for i, (sequences_batch, targets_batch) in enumerate(dataloader):\n",
    "    print(i)\n",
    "    print(f\" {sequences_batch.shape =}\")\n",
    "    print(f\" {targets_batch.shape =}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80c645-51a3-4671-ba8e-78bcdca61bfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data preprocessing v1 TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e043542-54a6-447a-8dec-d16c049c518d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T04:41:16.740567Z",
     "iopub.status.busy": "2024-10-19T04:41:16.740161Z",
     "iopub.status.idle": "2024-10-19T04:41:21.799022Z",
     "shell.execute_reply": "2024-10-19T04:41:21.798639Z",
     "shell.execute_reply.started": "2024-10-19T04:41:16.740538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(file_names) = 366\n",
      "..........len(file_paths) = 10\n",
      "len(file_paths) = 2\n",
      "Epoch: 0\n",
      ".torch.Size([32, 6])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "def _load_file(file_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Load a file and convert all values to a numeric type.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path, columns=['datetime', 'density'])\n",
    "    assert df.shape[0] == ROWS_IN_FILE, f\"File {file_path}, {df.shape[0]} != 9120912\"\n",
    "    if verbose:\n",
    "        print(f\"Loaded {df.shape} {file_path}\")\n",
    "    else:\n",
    "        print('.', end='')\n",
    "    return df\n",
    "\n",
    "    \n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths  # List of paths to pandas files\n",
    "        self.transform = transform\n",
    "        self.data = None  # Placeholder for loaded data\n",
    "\n",
    "    \n",
    "    def load_file(self, idx):\n",
    "        \"\"\"Load the pandas file at the given index.\"\"\"\n",
    "        file_path = self.file_paths[idx]\n",
    "        self.data = _load_file(file_path)\n",
    "        # print(self.data.tail())\n",
    "        if self.transform:\n",
    "            self.data = self.transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of rows across all files.\"\"\"\n",
    "        # total_length = 0\n",
    "        # for file in self.file_paths:\n",
    "        #     total_length += len(_load_file(file))  # Adjust based on file format\n",
    "        # return total_length\n",
    "        return 9120912 * len(self.file_paths) # Rows in file should always be 9120912 !\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single sample from the dataset.\"\"\"\n",
    "        # Identify which file the idx belongs to\n",
    "        file_idx = 0\n",
    "        while idx >= SAMPLES_IN_FILE: # It should be always 48 ! # len(_load_file(self.file_paths[file_idx])):\n",
    "            idx -= SAMPLES_IN_FILE # len(_load_file(self.file_paths[file_idx]))\n",
    "            file_idx += 1\n",
    "        \n",
    "        # Load the relevant file and fetch the row\n",
    "        if self.data is None or self.current_file_idx != file_idx:\n",
    "            self.load_file(file_idx)\n",
    "            self.current_file_idx = file_idx\n",
    "\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # convert pandas row to tensor\n",
    "        features = torch.tensor(row.iloc[:-1].values, dtype=torch.float32)  # Example for feature extraction\n",
    "        label = torch.tensor(row.iloc[-1], dtype=torch.float32)  # Example for label\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Paths to your pandas files\n",
    "# file_name = '2000-01-01.e744a8ed50e64f5b8f56d3bab9f2cd1d-0.parquet'\n",
    "# file_path = Path('../data/original/2000') / file_name\n",
    "# file_paths = [file_path]\n",
    "\n",
    "data_dir = '../data'\n",
    "original_data_dir = 'original'\n",
    "year = '2000'\n",
    "directory = Path(data_dir) / original_data_dir  / str(year)\n",
    "file_names = os.listdir(directory)[:10]  # DEBUG\n",
    "print(f'{len(file_names) = }')\n",
    "# files should be sorted!\n",
    "file_paths = [Path(data_dir) / original_data_dir  / str(year) / file_name for file_name in sorted(file_names)]\n",
    "file_paths = [pth for pth in file_paths if len(_load_file(pth)) == ROWS_IN_FILE]\n",
    "print(f'{len(file_paths) = }')\n",
    "                       \n",
    "file_paths = file_paths[:2] # DEBUG\n",
    "print(f'{len(file_paths) = }')\n",
    "\n",
    "# Create the Dataset\n",
    "\n",
    "dataset = PandasDataset(file_paths)\n",
    "num_epochs = 2\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Example usage in training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        print(inputs.shape)\n",
    "        print(labels.shape)\n",
    "        break\n",
    "    break\n",
    "        # Train your model with inputs and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32befa-e7f2-48ef-aae8-0f9fe8e6b535",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a78cfc-243b-47a1-8da9-6347c3f22289",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd64d2f4-19ba-4062-8b90-3a6d83016d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T21:53:52.345656Z",
     "iopub.status.busy": "2024-10-20T21:53:52.345220Z",
     "iopub.status.idle": "2024-10-20T21:53:52.352147Z",
     "shell.execute_reply": "2024-10-20T21:53:52.351738Z",
     "shell.execute_reply.started": "2024-10-20T21:53:52.345622Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :]\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, num_features, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.input_layer = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_features)  # Output same dimension as input (10 features)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_layer(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        transformer_output = self.transformer_encoder(src)\n",
    "        output = self.fc(transformer_output[:, -1, :])  # Use the last time step output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b7b9b-550a-4efc-840a-cc9e0c3fd1a1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cefd3164-252b-4fb4-97a9-14461371276b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T21:54:21.561547Z",
     "iopub.status.busy": "2024-10-20T21:54:21.561085Z",
     "iopub.status.idle": "2024-10-20T22:00:50.705999Z",
     "shell.execute_reply": "2024-10-20T22:00:50.705418Z",
     "shell.execute_reply.started": "2024-10-20T21:54:21.561504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................Loaded 50 files, (12000000, 2)\n",
      "Normalized len(density_normalized) = 1200, density_normalized[0] = array([1.0058001e-08, 5.5147993e-09, 3.2282998e-09, ..., 2.8465999e-12,\n",
      "       2.6720000e-12, 2.5087001e-12], dtype=float32)\n",
      "Created TensorDataset: <torch.utils.data.dataset.TensorDataset object at 0x7589c4f62c80>\n",
      "Created DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x7589c4f62bc0>, batch_size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0132\n",
      "Epoch [2/10], Loss: 0.0011\n",
      "Epoch [3/10], Loss: 0.0001\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "files = 50\n",
    "num_epochs = 10\n",
    "num_features = SAMPLE_SIZE  # Each 'datetime' has a vector of SAMPLE_SIZE 'density' values\n",
    "\n",
    "dataloader, density_normalized = get_dataloader(sequence_length=3, batch_size=4, tensor_dtype=torch.float32, verbose=False, files=files)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerTimeSeries(num_features=num_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Store loss values for visualization\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        sequences_batch, targets_batch = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.47}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04006614-e97e-4a7d-b5b0-85aa13b76960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ba3215c-5e4a-47b2-973e-805b5e015f87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:08:27.978131Z",
     "iopub.status.busy": "2024-10-20T22:08:27.977280Z",
     "iopub.status.idle": "2024-10-20T22:08:27.991218Z",
     "shell.execute_reply": "2024-10-20T22:08:27.990864Z",
     "shell.execute_reply.started": "2024-10-20T22:08:27.978059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted density vector: [[ 1.6741596e-05  1.7674180e-03  2.1249938e-03 ... -1.9194558e-03\n",
      "   1.0142568e-03 -5.0163362e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Inference (predicting the next density vector from the last available sequence)\n",
    "sequence_length = 3\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_sequence = torch.tensor(density_normalized[-sequence_length:], dtype=torch.float32).unsqueeze(0)\n",
    "    prediction = model(last_sequence)\n",
    "    prediction = scaler.inverse_transform(prediction.numpy())  # Inverse transform to original scale\n",
    "    print(\"Next predicted density vector:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750567e0-8bde-4484-9891-a0ea1356342a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
